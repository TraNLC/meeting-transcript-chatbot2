{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ WhisperX API Server - Zero Cost Meeting Transcription\n",
        "\n",
        "**H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**\n",
        "1. Ch·ªçn Runtime > Change runtime type > GPU (T4)\n",
        "2. Ch·∫°y t·∫•t c·∫£ cells (Runtime > Run all)\n",
        "3. App s·∫Ω t·ª± ƒë·ªông k·∫øt n·ªëi (n·∫øu m·ªü t·ª´ link Smart Connect)\n",
        "4. N·∫øu kh√¥ng t·ª± k·∫øt n·ªëi, Copy URL ngrok v√† d√°n v√†o App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoconnect"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup & Auto-Connect Logic\n",
        "import time\n",
        "from google.colab import output\n",
        "import IPython\n",
        "\n",
        "SESSION_ID = None\n",
        "\n",
        "def set_session_id(id_str):\n",
        "    global SESSION_ID\n",
        "    SESSION_ID = id_str\n",
        "    print(f\"üîó Auto-Connect Enabled! Session ID: {SESSION_ID}\")\n",
        "\n",
        "output.register_callback('notebook.set_session_id', set_session_id)\n",
        "\n",
        "js_code = \"\"\"\n",
        "try {\n",
        "    var hash = window.location.hash;\n",
        "    if (hash && hash.includes('session=')) {\n",
        "        var id = hash.split('session=')[1].split('&')[0];\n",
        "        google.colab.kernel.invokeFunction('notebook.set_session_id', [id], {});\n",
        "        document.body.insertAdjacentHTML('beforebegin', \n",
        "            '<div style=\"background:#dcfce7;color:#166534;padding:10px;text-align:center;font-weight:bold;font-size:16px\">' +\n",
        "            '‚úÖ Connected to Akari App (Session: ' + id + ')' +\n",
        "            '</div>');\n",
        "    }\n",
        "} catch (e) { console.error('Auto-connect error:', e); }\n",
        "\"\"\"\n",
        "display(IPython.display.Javascript(js_code))\n",
        "time.sleep(2) # Wait for JS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies (Fixed)\n",
        "!pip install whisperx flask flask-cors pyngrok python-dotenv requests -q\n",
        "!pip install faster-whisper -q\n",
        "\n",
        "# Fix Torch Version Mismatch (Pin to Stable 2.3.1)\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "print(\"‚úÖ C√†i ƒë·∫∑t ho√†n t·∫•t!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Configuration\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import whisperx\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "# ‚ö†Ô∏è C·∫§U H√åNH TOKEN (Ch·ªâ c·∫ßn l√†m 1 l·∫ßn)\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN\"  \n",
        "HF_TOKEN = \"YOUR_HF_TOKEN\" \n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "port = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Load Model (Smart Load)\n",
        "import gc\n",
        "\n",
        "# 1. Cleanup Memory First\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 16\n",
        "\n",
        "# --- MONKEY PATCH FIX FOR TORCH 2.6+ ---\n",
        "try:\n",
        "    _original_torch_load = torch.load\n",
        "    def safe_torch_load(*args, **kwargs):\n",
        "        if 'weights_only' not in kwargs:\n",
        "            kwargs['weights_only'] = False\n",
        "        return _original_torch_load(*args, **kwargs)\n",
        "    torch.load = safe_torch_load\n",
        "    print(\"üîß Applied 'weights_only=False' patch for Torch load\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not patch torch.load: {e}\")\n",
        "# ---------------------------------------\n",
        "\n",
        "print(f\"üéØ Device: {device}\")\n",
        "\n",
        "try:\n",
        "    # Try loading with float16 (Best Quality)\n",
        "    compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "    print(\"üì¶ ƒêang load WhisperX model (large-v3) [float16]...\")\n",
        "    model = whisperx.load_model(\"large-v3\", device, compute_type=compute_type)\n",
        "    print(\"‚úÖ Model loaded successfully (float16)!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"out of memory\" in error_msg or \"CUDA out of memory\" in error_msg:\n",
        "        print(\"‚ö†Ô∏è H·∫øt b·ªô nh·ªõ VRAM v·ªõi float16. ƒêang chuy·ªÉn sang int8 (Nh·∫π h∆°n)...\")\n",
        "        # Cleanup again\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        # Fallback to int8\n",
        "        compute_type = \"int8\"\n",
        "        model = whisperx.load_model(\"large-v3\", device, compute_type=compute_type)\n",
        "        print(\"‚úÖ Model loaded successfully (int8)!\")\n",
        "    else:\n",
        "        # Re-raise other errors\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api"
      },
      "outputs": [],
      "source": [
        "# Cell 5: API Controller\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\", \"device\": device, \"compute_type\": compute_type})\n",
        "\n",
        "@app.route('/transcribe', methods=['POST'])\n",
        "def transcribe_audio():\n",
        "    try:\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "        \n",
        "        file = request.files['file']\n",
        "        if file.filename == '':\n",
        "            return jsonify({\"error\": \"Empty filename\"}), 400\n",
        "        \n",
        "        # Check language param\n",
        "        language = request.form.get('language')\n",
        "        \n",
        "        # Save temp file\n",
        "        temp_path = \"/tmp/temp_audio.wav\"\n",
        "        file.save(temp_path)\n",
        "        \n",
        "        print(f\"üìù Processing: {file.filename} (Lang: {language})\")\n",
        "        \n",
        "        # 1. Load audio\n",
        "        audio = whisperx.load_audio(temp_path)\n",
        "        \n",
        "        # 2. Transcribe\n",
        "        options = {\"batch_size\": batch_size}\n",
        "        if language and language not in ['auto', '']:\n",
        "            options[\"language\"] = language\n",
        "\n",
        "        result = model.transcribe(audio, **options)\n",
        "        \n",
        "        # 3. Align\n",
        "        if result[\"segments\"]:\n",
        "            model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "            result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "        \n",
        "        # 4. Diarization\n",
        "        if HF_TOKEN and HF_TOKEN != \"YOUR_HF_TOKEN\":\n",
        "            try:\n",
        "                diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
        "                diarize_segments = diarize_model(audio)\n",
        "                result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Diarization error: {e}\")\n",
        "        \n",
        "        # Cleanup\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "        if os.path.exists(temp_path): os.remove(temp_path)\n",
        "        \n",
        "        return jsonify({\"success\": True, \"segments\": result[\"segments\"]})\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return jsonify({\"error\": str(e)}), 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Run Server & Sync\n",
        "import requests\n",
        "\n",
        "if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\"\\nüöÄ Server Running at: {public_url}\")\n",
        "    \n",
        "    # AUTO-SYNC logic (Disabled to prevent errors)\n",
        "    # if SESSION_ID:\n",
        "    #     try:\n",
        "    #         requests.post(f\"https://dweet.io/dweet/for/{SESSION_ID}\", json={\"url\": public_url, \"status\": \"ready\"})\n",
        "    #     except Exception as e: pass\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Missing Ngrok Token. Please update Cell 3.\")\n",
        "\n",
        "app.run(port=port, debug=False, use_reloader=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}