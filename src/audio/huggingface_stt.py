"""HuggingFace Speech-to-Text using Whisper model.

Uses transformers pipeline for easy integration.
"""

from pathlib import Path
from datetime import datetime


def transcribe_audio_huggingface(audio_file, language="vi"):
    """Transcribe audio using HuggingFace Whisper model.
    
    Args:
        audio_file: Path to audio file
        language: Language code (vi, en, ja, ko, zh)
        
    Yields:
        str: Progress messages and final transcript
    """
    if audio_file is None or audio_file == "":
        yield "üéôÔ∏è S·∫µn s√†ng ghi √¢m. Nh·∫•n microphone icon ƒë·ªÉ b·∫Øt ƒë·∫ßu..."
        return
    
    # Debug: Log file path
    print(f"[DEBUG] Transcribing file: {audio_file}")
    print(f"[DEBUG] Language: {language}")
    
    try:
        from transformers import pipeline
        import torch
        import librosa
        import shutil
        import tempfile
        
        yield "üîÑ ƒêang kh·ªüi t·∫°o HuggingFace Whisper..."
        
        # Create a unique copy of audio file to avoid Gradio cache
        # This ensures each recording is processed fresh
        temp_dir = tempfile.gettempdir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        unique_audio_path = Path(temp_dir) / f"whisper_input_{timestamp}.wav"
        
        # Copy audio file to unique path
        shutil.copy2(audio_file, unique_audio_path)
        audio_file = str(unique_audio_path)  # Use unique path for processing
        
        print(f"[DEBUG] Using unique file: {audio_file}")
        
        # Check file size and duration
        file_path = Path(audio_file)
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        
        # Get audio duration
        try:
            audio_data, sr = librosa.load(audio_file, sr=None)
            duration_sec = len(audio_data) / sr
            duration_min = duration_sec / 60
            
            yield f"üìä File: {file_size_mb:.1f} MB, Th·ªùi l∆∞·ª£ng: {duration_min:.1f} ph√∫t"
            
            # Estimate processing time (rough estimate)
            # Medium model on CPU: ~1x realtime (1 min audio = 1 min processing)
            # On GPU: ~5-10x faster
            device = 0 if torch.cuda.is_available() else -1
            device_name = "GPU (CUDA)" if device == 0 else "CPU"
            
            if device == 0:
                est_time = duration_min / 5  # GPU is ~5x faster
                speed_info = "nhanh 5x"
            else:
                est_time = duration_min * 1.2  # CPU is ~1.2x realtime
                speed_info = "realtime"
            
            # Format time estimate
            if est_time < 1:
                time_str = f"{int(est_time * 60)} gi√¢y"
            else:
                time_str = f"{est_time:.1f} ph√∫t"
            
            yield f"‚öôÔ∏è  Thi·∫øt b·ªã: {device_name} ({speed_info}) | D·ª± ki·∫øn: ~{time_str}"
            
            # Check if file too large with detailed warnings
            if duration_min > 120:
                yield f"üö® C·∫£nh b√°o: File r·∫•t d√†i ({duration_min:.1f} ph√∫t = {duration_min/60:.1f} gi·ªù). Khuy·∫øn ngh·ªã chia nh·ªè file!"
            elif duration_min > 60:
                yield f"‚ö†Ô∏è  C·∫£nh b√°o: File d√†i ({duration_min:.1f} ph√∫t). C√≥ th·ªÉ m·∫•t {time_str} ƒë·ªÉ x·ª≠ l√Ω..."
            
            if file_size_mb > 200:
                yield f"üö® C·∫£nh b√°o: File r·∫•t l·ªõn ({file_size_mb:.1f} MB). C·∫ßn √≠t nh·∫•t 4GB RAM!"
            elif file_size_mb > 100:
                yield f"‚ö†Ô∏è  C·∫£nh b√°o: File l·ªõn ({file_size_mb:.1f} MB). ƒê·∫£m b·∫£o ƒë·ªß RAM (khuy·∫øn ngh·ªã 2GB+)..."
                
        except Exception as e:
            yield f"‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c th√¥ng tin file: {e}"
        
        # Load pipeline (force reload to avoid cache)
        yield "üì• ƒêang t·∫£i Whisper model t·ª´ HuggingFace..."
        
        # Use openai/whisper-base model for faster processing
        # Base model: 74M params - faster and good enough for most cases
        # For long audio (>30s), we need return_timestamps=True
        import gc
        gc.collect()  # Clear memory before loading
        
        # Use small model for better accuracy (still fast enough)
        # base: 74M params, small: 244M params (3x larger, more accurate)
        pipe = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-small",  # Better accuracy than base
            device=device,
            return_timestamps=True,  # Required for long-form audio
            torch_dtype=torch.float32  # Ensure consistent dtype
        )
        
        yield f"üé§ ƒêang transcribe audio (ng√¥n ng·ªØ: {language})..."
        
        # Transcribe with progress
        import time
        import hashlib
        start_time = time.time()
        
        # Calculate file hash to ensure unique processing
        with open(audio_file, 'rb') as f:
            audio_bytes = f.read()
            file_hash = hashlib.md5(audio_bytes).hexdigest()[:8]
            file_size_bytes = len(audio_bytes)
        
        yield f"üîç Processing audio (ID: {file_hash}, Size: {file_size_bytes} bytes)..."
        
        # Check if this is a known cached file
        known_hashes = {
            "80d4ec5a": "Ghi·ªÅn M√¨ G√µ ƒê·∫Ω sample",  # Known sample file
        }
        
        if file_hash in known_hashes:
            yield f"‚ö†Ô∏è  WARNING: ƒê√¢y c√≥ th·ªÉ l√† file m·∫´u cached: {known_hashes[file_hash]}"
            yield f"‚ö†Ô∏è  H√£y th·ª≠: 1) Clear browser cache, 2) Refresh page, 3) Record l·∫°i"
        
        result = pipe(
            audio_file,
            generate_kwargs={
                "language": language,
                "task": "transcribe",
                "temperature": 0.0,  # More deterministic, reduce hallucination
                "no_repeat_ngram_size": 3  # Prevent repetition
            }
        )
        
        elapsed_time = time.time() - start_time
        
        # Extract text from result (handle both formats)
        if isinstance(result, dict):
            if "text" in result:
                transcript = result["text"].strip()
            elif "chunks" in result:
                # Combine chunks if timestamps are returned
                transcript = " ".join([chunk["text"] for chunk in result["chunks"]]).strip()
            else:
                transcript = str(result).strip()
        else:
            transcript = str(result).strip()
        
        # Detect potential hallucination (very short or repetitive text)
        if len(transcript) < 10:
            yield f"‚ö†Ô∏è  Warning: Transcript qu√° ng·∫Øn ({len(transcript)} chars). Audio c√≥ th·ªÉ kh√¥ng r√µ ho·∫∑c qu√° ng·∫Øn."
        
        # Check for repetition (hallucination indicator)
        words = transcript.split()
        if len(words) > 5:
            unique_words = len(set(words))
            repetition_ratio = unique_words / len(words)
            if repetition_ratio < 0.3:  # Less than 30% unique words
                yield f"‚ö†Ô∏è  Warning: Ph√°t hi·ªán l·∫∑p t·ª´ nhi·ªÅu (c√≥ th·ªÉ hallucination). Th·ª≠ record l·∫°i v·ªõi audio r√µ h∆°n."
        
        # Get timestamp
        now = datetime.now().strftime("%d/%m/%Y %H:%M")
        
        # Calculate speed if duration available
        speed_info = ""
        try:
            if duration_sec > 0:
                speed_factor = duration_sec / elapsed_time
                speed_info = f" | T·ªëc ƒë·ªô: {speed_factor:.1f}x realtime"
        except:
            pass
        
        yield f"""üìù **Transcript ({language}):** {now}

{transcript}

‚úÖ Ho√†n th√†nh trong {elapsed_time:.1f}s{speed_info}
"""
        
        # Cleanup unique temp file
        try:
            if unique_audio_path.exists():
                unique_audio_path.unlink()
                print(f"[DEBUG] Cleaned up temp file: {unique_audio_path}")
        except:
            pass
            
    except ImportError as e:
        yield f"""‚ùå L·ªói: Ch∆∞a c√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt

üí° **C√†i ƒë·∫∑t HuggingFace Transformers:**

```bash
pip install transformers torch torchaudio
```

**V·ªõi GPU support (khuy·∫øn ngh·ªã):**
```bash
pip install transformers torch torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Mi·ªÖn ph√≠, kh√¥ng c·∫ßn API key
- ‚úÖ Ch·∫°y offline (local)
- ‚úÖ H·ªó tr·ª£ 50+ ng√¥n ng·ªØ
- ‚úÖ D·ªÖ c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng

**L·ªói chi ti·∫øt:** {str(e)}
"""
    except Exception as e:
        yield f"""‚ùå L·ªói khi transcribe: {str(e)}

üí° **Ki·ªÉm tra:**
- File audio c√≥ h·ª£p l·ªá kh√¥ng?
- ƒê√£ c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß th∆∞ vi·ªán ch∆∞a?
- C√≥ ƒë·ªß RAM kh√¥ng? (c·∫ßn ~2GB cho base model)

**Th·ª≠ l·∫°i ho·∫∑c li√™n h·ªá support.**
"""


def transcribe_audio_simple(audio_file, language="vi"):
    """Simple non-generator version for testing.
    
    Args:
        audio_file: Path to audio file
        language: Language code
        
    Returns:
        str: Transcript text
    """
    if audio_file is None or audio_file == "":
        return "üéôÔ∏è Ch∆∞a c√≥ audio ƒë·ªÉ transcribe"
    
    try:
        from transformers import pipeline
        import torch
        
        device = 0 if torch.cuda.is_available() else -1
        
        pipe = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-base",
            device=device
        )
        
        result = pipe(
            audio_file,
            generate_kwargs={
                "language": language,
                "task": "transcribe"
            }
        )
        
        transcript = result["text"].strip()
        now = datetime.now().strftime("%d/%m/%Y %H:%M")
        
        return f"""üìù **Transcript ({language}):** {now}

{transcript}
"""
        
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"
