"""HuggingFace Speech-to-Text using Whisper model.

Uses transformers pipeline for easy integration.
"""

from pathlib import Path
from datetime import datetime


def transcribe_audio_huggingface(audio_file, language="vi"):
    """Transcribe audio using HuggingFace Whisper model.
    
    Args:
        audio_file: Path to audio file
        language: Language code (vi, en, ja, ko, zh)
        
    Yields:
        str: Progress messages and final transcript
    """
    if audio_file is None or audio_file == "":
        yield "ğŸ™ï¸ Sáºµn sÃ ng ghi Ã¢m. Nháº¥n microphone icon Ä‘á»ƒ báº¯t Ä‘áº§u..."
        return
    
    try:
        from transformers import pipeline
        import torch
        import librosa
        
        yield "ğŸ”„ Äang khá»Ÿi táº¡o HuggingFace Whisper..."
        
        # Check file size and duration
        file_path = Path(audio_file)
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        
        # Get audio duration
        try:
            audio_data, sr = librosa.load(audio_file, sr=None)
            duration_sec = len(audio_data) / sr
            duration_min = duration_sec / 60
            
            yield f"ğŸ“Š File: {file_size_mb:.1f} MB, Thá»i lÆ°á»£ng: {duration_min:.1f} phÃºt"
            
            # Estimate processing time (rough estimate)
            # Medium model on CPU: ~1x realtime (1 min audio = 1 min processing)
            # On GPU: ~5-10x faster
            device = 0 if torch.cuda.is_available() else -1
            device_name = "GPU (CUDA)" if device == 0 else "CPU"
            
            if device == 0:
                est_time = duration_min / 5  # GPU is ~5x faster
                speed_info = "nhanh 5x"
            else:
                est_time = duration_min * 1.2  # CPU is ~1.2x realtime
                speed_info = "realtime"
            
            # Format time estimate
            if est_time < 1:
                time_str = f"{int(est_time * 60)} giÃ¢y"
            else:
                time_str = f"{est_time:.1f} phÃºt"
            
            yield f"âš™ï¸  Thiáº¿t bá»‹: {device_name} ({speed_info}) | Dá»± kiáº¿n: ~{time_str}"
            
            # Check if file too large with detailed warnings
            if duration_min > 120:
                yield f"ğŸš¨ Cáº£nh bÃ¡o: File ráº¥t dÃ i ({duration_min:.1f} phÃºt = {duration_min/60:.1f} giá»). Khuyáº¿n nghá»‹ chia nhá» file!"
            elif duration_min > 60:
                yield f"âš ï¸  Cáº£nh bÃ¡o: File dÃ i ({duration_min:.1f} phÃºt). CÃ³ thá»ƒ máº¥t {time_str} Ä‘á»ƒ xá»­ lÃ½..."
            
            if file_size_mb > 200:
                yield f"ğŸš¨ Cáº£nh bÃ¡o: File ráº¥t lá»›n ({file_size_mb:.1f} MB). Cáº§n Ã­t nháº¥t 4GB RAM!"
            elif file_size_mb > 100:
                yield f"âš ï¸  Cáº£nh bÃ¡o: File lá»›n ({file_size_mb:.1f} MB). Äáº£m báº£o Ä‘á»§ RAM (khuyáº¿n nghá»‹ 2GB+)..."
                
        except Exception as e:
            yield f"âš ï¸  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c thÃ´ng tin file: {e}"
        
        # Load pipeline
        yield "ğŸ“¥ Äang táº£i Whisper model tá»« HuggingFace..."
        
        # Use openai/whisper-medium model (high accuracy)
        # Medium model: 769M params - best balance between accuracy and speed
        pipe = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-medium",
            device=device,
            chunk_length_s=30,  # Process in 30s chunks for better accuracy
            return_timestamps=False
        )
        
        yield f"ğŸ¤ Äang transcribe audio (ngÃ´n ngá»¯: {language})..."
        
        # Transcribe with progress
        import time
        start_time = time.time()
        
        # Show progress updates during transcription
        yield f"â³ Äang xá»­ lÃ½... (0%)"
        
        result = pipe(
            audio_file,
            generate_kwargs={
                "language": language,
                "task": "transcribe"
            }
        )
        
        elapsed_time = time.time() - start_time
        
        # Calculate actual speed
        if duration_sec > 0:
            speed_factor = duration_sec / elapsed_time
            yield f"âœ… HoÃ n thÃ nh! Tá»‘c Ä‘á»™: {speed_factor:.1f}x realtime ({elapsed_time:.1f}s cho {duration_min:.1f} phÃºt audio)"
        
        transcript = result["text"].strip()
        
        # Get timestamp
        now = datetime.now().strftime("%d/%m/%Y %H:%M")
        
        yield f"""ğŸ“ **Transcript ({language}):** {now}

{transcript}

âœ… HoÃ n thÃ nh trong {elapsed_time:.1f} giÃ¢y! Sá»­ dá»¥ng HuggingFace Whisper (Medium model)
"""
            
    except ImportError as e:
        yield f"""âŒ Lá»—i: ChÆ°a cÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t

ğŸ’¡ **CÃ i Ä‘áº·t HuggingFace Transformers:**

```bash
pip install transformers torch torchaudio
```

**Vá»›i GPU support (khuyáº¿n nghá»‹):**
```bash
pip install transformers torch torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Miá»…n phÃ­, khÃ´ng cáº§n API key
- âœ… Cháº¡y offline (local)
- âœ… Há»— trá»£ 50+ ngÃ´n ngá»¯
- âœ… Dá»… cÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng

**Lá»—i chi tiáº¿t:** {str(e)}
"""
    except Exception as e:
        yield f"""âŒ Lá»—i khi transcribe: {str(e)}

ğŸ’¡ **Kiá»ƒm tra:**
- File audio cÃ³ há»£p lá»‡ khÃ´ng?
- ÄÃ£ cÃ i Ä‘áº·t Ä‘áº§y Ä‘á»§ thÆ° viá»‡n chÆ°a?
- CÃ³ Ä‘á»§ RAM khÃ´ng? (cáº§n ~2GB cho base model)

**Thá»­ láº¡i hoáº·c liÃªn há»‡ support.**
"""


def transcribe_audio_simple(audio_file, language="vi"):
    """Simple non-generator version for testing.
    
    Args:
        audio_file: Path to audio file
        language: Language code
        
    Returns:
        str: Transcript text
    """
    if audio_file is None or audio_file == "":
        return "ğŸ™ï¸ ChÆ°a cÃ³ audio Ä‘á»ƒ transcribe"
    
    try:
        from transformers import pipeline
        import torch
        
        device = 0 if torch.cuda.is_available() else -1
        
        pipe = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-base",
            device=device
        )
        
        result = pipe(
            audio_file,
            generate_kwargs={
                "language": language,
                "task": "transcribe"
            }
        )
        
        transcript = result["text"].strip()
        now = datetime.now().strftime("%d/%m/%Y %H:%M")
        
        return f"""ğŸ“ **Transcript ({language}):** {now}

{transcript}
"""
        
    except Exception as e:
        return f"âŒ Lá»—i: {str(e)}"
